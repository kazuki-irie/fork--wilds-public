import os, csv
import time
import argparse
import torch
import torch.nn as nn
import torchvision
import sys
from collections import defaultdict

import wilds
from wilds.common.data_loaders import get_train_loader, get_eval_loader
from wilds.common.grouper import CombinatorialGrouper

from utils import (
    set_seed, Logger, BatchLogger, log_config, ParseKwargs, load, load_custom,
    initialize_wandb, log_group_data, parse_bool, get_model_prefix)
from train import train, evaluate
from algorithms.initializer import initialize_algorithm
from transforms import initialize_transform
from configs.utils import populate_defaults
import configs.supported as supported

import torch.multiprocessing


def main():

    ''' to see default hyperparams for each dataset/model, look at configs/ '''
    parser = argparse.ArgumentParser()

    # Required arguments
    parser.add_argument(
        '-d', '--dataset', choices=wilds.supported_datasets, required=True)
    parser.add_argument(
        '--algorithm', required=True, choices=supported.algorithms)
    parser.add_argument(
        '--root_dir', required=True,
        help='The directory where [dataset]/data can be found (or should be '
             'downloaded to, if it does not exist).')

    # Dataset
    parser.add_argument(
        '--split_scheme',
        help='Identifies how the train/val/test split is constructed. '
             'Choices are dataset-specific.')
    parser.add_argument(
        '--dataset_kwargs', nargs='*', action=ParseKwargs, default={})
    parser.add_argument(
        '--download', default=False, type=parse_bool, const=True, nargs='?',
        help='If True, downloads the dataset if not existing in root_dir.')
    parser.add_argument(
        '--frac', type=float, default=1.0,
        help='Convenience parameter that scales all dataset splits down to '
             'the specified fraction, for development purposes. '
             'Note that this also scales the test set down, so the reported '
             'numbers are not comparable with the full test set.')
    parser.add_argument('--version', default=None, type=str)

    # Loaders
    parser.add_argument('--loader_kwargs',
                        nargs='*', action=ParseKwargs, default={})
    parser.add_argument('--train_loader', choices=['standard', 'group'])
    parser.add_argument('--uniform_over_groups',
                        type=parse_bool, const=True, nargs='?')
    parser.add_argument('--distinct_groups',
                        type=parse_bool, const=True, nargs='?')
    parser.add_argument('--n_groups_per_batch', type=int)
    parser.add_argument('--n_sequences_per_batch', type=int)

    parser.add_argument('--batch_size', type=int)
    parser.add_argument('--bptt_len', type=int)
    parser.add_argument('--eval_loader',
                        choices=['standard'], default='standard')

    # Model
    parser.add_argument('--model', choices=supported.models)
    parser.add_argument('--report_ppl', type=parse_bool, const=True, nargs='?',
                        help="Report exp(objective), mainly for py150")

    parser.add_argument('--model_kwargs',
                        nargs='*', action=ParseKwargs, default={},
                        help='keyword arguments for model initialization '
                             'passed as key1=value1 key2=value2')

    # Transforms
    parser.add_argument('--transform', choices=supported.transforms)
    parser.add_argument('--target_resolution',
                        nargs='+', type=int,
                        help='The input resolution that images will be '
                             'resized to before being passed into the model. '
                             'For example, use --target_resolution 224 224 '
                             'for a standard ResNet.')
    parser.add_argument('--resize_scale', type=float)
    parser.add_argument('--max_token_length', type=int)

    # Objective
    parser.add_argument('--loss_function', choices=supported.losses)
    parser.add_argument('--loss_kwargs',
                        nargs='*', action=ParseKwargs, default={},
                        help='keyword arguments for loss initialization '
                             'passed as key1=value1 key2=value2')

    # Algorithm
    parser.add_argument('--groupby_fields', nargs='+')
    parser.add_argument('--group_dro_step_size', type=float)
    parser.add_argument('--coral_penalty_weight', type=float)
    parser.add_argument('--irm_lambda', type=float)
    parser.add_argument('--irm_penalty_anneal_iters', type=int)
    parser.add_argument('--algo_log_metric')

    # Model selection
    parser.add_argument('--val_metric')
    parser.add_argument('--sub_val_metric')
    parser.add_argument('--val_metric_decreasing',
                        type=parse_bool, const=True, nargs='?')

    # Optimization
    parser.add_argument('--n_epochs', type=int)
    parser.add_argument('--optimizer', choices=supported.optimizers)
    parser.add_argument('--lr', type=float)
    parser.add_argument('--grad_acc', type=int, default=1,
                        help='gradient accumulation steps')
    parser.add_argument('--weight_decay', type=float)
    parser.add_argument('--max_grad_norm', type=float)
    parser.add_argument('--optimizer_kwargs',
                        nargs='*', action=ParseKwargs, default={})

    # Scheduler
    parser.add_argument('--scheduler', choices=supported.schedulers)
    parser.add_argument('--scheduler_kwargs',
                        nargs='*', action=ParseKwargs, default={})
    parser.add_argument('--scheduler_metric_split',
                        choices=['train', 'val'], default='val')
    parser.add_argument('--scheduler_metric_name')

    # Evaluation
    parser.add_argument('--process_outputs_function',
                        choices=supported.process_outputs_functions)
    parser.add_argument('--evaluate_all_splits',
                        type=parse_bool, const=True, nargs='?', default=True)
    parser.add_argument('--eval_splits', nargs='+', default=[])
    parser.add_argument('--eval_only',
                        type=parse_bool, const=True, nargs='?', default=False)
    parser.add_argument('--skip_train_eval',
                        type=parse_bool, const=True, nargs='?', default=True)
    parser.add_argument('--eval_carryover',
                        type=parse_bool, const=True, nargs='?', default=False)
    parser.add_argument('--eval_epoch', default=None, type=int,
                        help='If eval_only is set, then eval_epoch allows you '
                             'to specify evaluating at a particular epoch. '
                             'By default, it evaluates the best epoch by '
                             'validation performance.')

    # Misc
    parser.add_argument('--device', type=int, default=0)
    parser.add_argument('--seed', type=int, default=0)
    parser.add_argument('--log_dir', default='./logs')
    parser.add_argument('--log_every', default=50, type=int)
    parser.add_argument('--validate_every', default=1000, type=int)
    parser.add_argument('--save_step', type=int)
    parser.add_argument('--save_best',
                        type=parse_bool, const=True, nargs='?', default=True)
    parser.add_argument('--save_last',
                        type=parse_bool, const=True, nargs='?', default=True)
    # changed the default to False
    parser.add_argument('--save_pred',
                        type=parse_bool, const=False, nargs='?', default=True)
    parser.add_argument('--to_out_device',
                        type=parse_bool, const=True, nargs='?', default=True,
                        help='See code! No need to be touched in general')
    parser.add_argument('--no_group_logging',
                        type=parse_bool, const=True, nargs='?')
    parser.add_argument('--use_wandb',
                        type=parse_bool, const=True, nargs='?', default=False)
    parser.add_argument('--progress_bar',
                        type=parse_bool, const=True, nargs='?', default=False)
    parser.add_argument('--resume',
                        type=parse_bool, const=True, nargs='?', default=False)

    config = parser.parse_args()
    config = populate_defaults(config)

    # For the GlobalWheat detection dataset,
    # we need to change the multiprocessing strategy or there will be
    # too many open file descriptors.
    if config.dataset == 'globalwheat':
        torch.multiprocessing.set_sharing_strategy('file_system')

    if config.dataset == 'py150':
        # to avoid computing argmax on cpu
        # ideally this should be set to False for all cases but some datasets
        # require to compute group stats on cpu/numpy
        config.to_out_device = False
        config.report_ppl = True  # report ppl for lm task

    # Set device
    if torch.cuda.is_available():
        config.device = torch.device("cuda:" + str(config.device))
    else:
        print("No GPU detected. Something went wrong.")
        sys.exit(0)
        config.device = torch.device("cpu")

    # Initialize logs
    if os.path.exists(config.log_dir) and config.resume:
        resume=True
        mode='a'
    elif os.path.exists(config.log_dir) and config.eval_only:
        resume=False
        mode='a'
    else:
        resume=False
        mode='w'

    if not os.path.exists(config.log_dir):
        os.makedirs(config.log_dir)
    logger = Logger(os.path.join(config.log_dir, 'log.txt'), mode)

    # Record config
    log_config(config, logger)

    # Set random seed
    set_seed(config.seed)

    # Data
    full_dataset = wilds.get_dataset(
        dataset=config.dataset,
        version=config.version,
        root_dir=config.root_dir,
        download=config.download,
        split_scheme=config.split_scheme,
        **config.dataset_kwargs)

    # To implement data augmentation (i.e., have different transforms
    # at training time vs. test time), modify these two lines:
    train_transform = initialize_transform(
        transform_name=config.transform,
        config=config,
        dataset=full_dataset,
        is_training=True)
    eval_transform = initialize_transform(
        transform_name=config.transform,
        config=config,
        dataset=full_dataset,
        is_training=False)

    train_grouper = CombinatorialGrouper(
        dataset=full_dataset,
        groupby_fields=config.groupby_fields)

    datasets = defaultdict(dict)
    for split in full_dataset.split_dict.keys():
        if split == 'train':
            transform = train_transform
            verbose = True
        elif split == 'val':
            transform = eval_transform
            verbose = True
        else:
            transform = eval_transform
            verbose = False
        # Get subset
        datasets[split]['dataset'] = full_dataset.get_subset(
            split,
            frac=config.frac,
            transform=transform)

        if split == 'train':
            datasets[split]['loader'] = get_train_loader(
                loader=config.train_loader,
                dataset=datasets[split]['dataset'],
                batch_size=config.batch_size,
                uniform_over_groups=config.uniform_over_groups,
                grouper=train_grouper,
                distinct_groups=config.distinct_groups,  # bool
                n_groups_per_batch=config.n_groups_per_batch,
                **config.loader_kwargs)
        else:
            datasets[split]['loader'] = get_eval_loader(
                loader=config.eval_loader,
                dataset=datasets[split]['dataset'],
                grouper=train_grouper,
                batch_size=config.batch_size,
                **config.loader_kwargs)

        # Set fields
        datasets[split]['split'] = split
        datasets[split]['name'] = full_dataset.split_names[split]
        datasets[split]['verbose'] = verbose

        # Loggers
        datasets[split]['eval_logger'] = BatchLogger(
            os.path.join(config.log_dir, f'{split}_eval.csv'),
            mode=mode, use_wandb=(config.use_wandb and verbose))

        datasets[split]['algo_logger'] = BatchLogger(
            os.path.join(config.log_dir, f'{split}_algo.csv'), mode=mode,
            use_wandb=(config.use_wandb and verbose))

        if config.use_wandb:
            initialize_wandb(config)

    # Logging dataset info
    # Show class breakdown if feasible
    if (config.no_group_logging and full_dataset.is_classification
        and full_dataset.y_size==1 and full_dataset.n_classes <= 10):

        log_grouper = CombinatorialGrouper(
            dataset=full_dataset,
            groupby_fields=['y'])

    elif config.no_group_logging:
        log_grouper = None
    else:
        log_grouper = train_grouper
    log_group_data(datasets, log_grouper, logger)

    ## Initialize algorithm
    algorithm = initialize_algorithm(
        config=config,
        datasets=datasets,
        train_grouper=train_grouper)

    model_prefix = get_model_prefix(datasets['train'], config)
    if not config.eval_only:
        ## Load saved results if resuming
        resume_success = False
        if resume:
            save_path = model_prefix + 'epoch:last_model.pth'
            if not os.path.exists(save_path):
                epochs = [
                    int(file.split('epoch:')[1].split('_')[0])
                    for file in os.listdir(config.log_dir) if file.endswith('.pth')]
                if len(epochs) > 0:
                    latest_epoch = max(epochs)
                    save_path = model_prefix + f'epoch:{latest_epoch}_model.pth'
            try:
                prev_epoch, best_val_metric = load(algorithm, save_path)
                epoch_offset = prev_epoch + 1
                logger.write(f'Resuming from epoch {epoch_offset} with best '
                             f'val metric {best_val_metric}')
                resume_success = True
            except FileNotFoundError:
                pass

        if resume_success is False:
            epoch_offset=0
            best_val_metric=None

        train(
            algorithm=algorithm,
            datasets=datasets,
            general_logger=logger,
            config=config,
            epoch_offset=epoch_offset,
            best_val_metric=best_val_metric)
    else:
        if config.eval_epoch is None:
            eval_model_path = model_prefix + 'epoch:best_model.pth'
        else:
            eval_model_path = (
                model_prefix + f'epoch:{config.eval_epoch}_model.pth')
        if config.eval_carryover:
            best_epoch, best_val_metric = load_custom(
                algorithm, eval_model_path)
        else:
            best_epoch, best_val_metric = load(algorithm, eval_model_path)
        if config.eval_epoch is None:
            epoch = best_epoch
        else:
            epoch = config.eval_epoch
        if epoch == best_epoch:
            is_best = True

        evaluate(
            algorithm=algorithm,
            datasets=datasets,
            epoch=epoch,
            general_logger=logger,
            config=config,
            is_best=is_best)

        if config.sub_val_metric is not None:
            logger.write('== Eval checkpoint best for sub metric ==\n')
            eval_model_path = model_prefix + 'epoch:sub_best_model.pth'
            if config.eval_carryover:
                best_epoch, best_val_metric = load_custom(
                    algorithm, eval_model_path)
            else:
                best_epoch, best_val_metric = load(algorithm, eval_model_path)
            if config.eval_epoch is None:
                epoch = best_epoch
            else:
                epoch = config.eval_epoch
            if epoch == best_epoch:
                is_best = True
            evaluate(
                algorithm=algorithm,
                datasets=datasets,
                epoch=epoch,
                general_logger=logger,
                config=config,
                is_best=None)        

    logger.close()
    for split in datasets:
        datasets[split]['eval_logger'].close()
        datasets[split]['algo_logger'].close()


if __name__ == '__main__':
    main()
